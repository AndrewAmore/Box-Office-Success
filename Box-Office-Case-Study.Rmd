---
title: \vspace{-1.5cm} Factors Effecting Box Office Success
# subtitle: "A STA610 Case Study"
author: "Andrew Amore"
date: "2022-10-14"
output: pdf_document
geometry: "left=1.75cm,right=1.75cm,top=1.5cm,bottom=1.5cm"
---
\vspace{-1cm}

```{r setup, include=FALSE}
knitr::opts_chunk$set(include=FALSE, echo=FALSE, message=FALSE, warning = FALSE, fig.align = "center")

library(tidyverse)  # data manipulation
library(corrplot)   # correlation plots
library(rcompanion) # correlation computation with categorical variables
library(pander)     # pretty printing tables
library(sentimentr) # sentiment analysis
# library(mice)       # missing data imputation
library(ggpubr)     # panel plotting
library(lme4)       # model fitting
library(lmerTest)   # p-values from mixed effect modeling
library(jtools)     # analyze regression results
library(scales)     # table formatting (commas)
library(merTools)   # investigate mixed effects
```

<!-- read in raw data and configure fields -->
```{r, read-in-data}
df = read.csv(file="./data/United_States_Film_Releases_2019.csv", 
              na.strings = c("","N/A"))

# remove '$' and ',' from fields
df$Box.Office = gsub("[$,]", "", df$Box.Office)
df$Budget = gsub("[$,]", "", df$Budget)

# clean up genre
## remove spaces
df$Genre = gsub(" ", "", df$Genre, fixed = TRUE)
## create field with the number of genres
df$Num.Genres = str_count(df$Genre, "/") + 1
## assume genres listed in order of "priority"...only take the top two genres
df = df %>% 
  separate(col=Genre, into=c("Genre.1", "Genre.2"), sep="/", extra="drop", fill="right", remove=FALSE)
df$Genre.2[is.na(df$Genre.2)] = "Not-Applicable"

# convert to desired types
df$Box.Office = as.numeric(df$Box.Office)
df$Budget = as.numeric(df$Budget)
## should try and tie factor levels here so they get assigned the same level
df$Genre.1 = as.factor(df$Genre.1)
df$Genre.2 = as.factor(df$Genre.2)
df$Release.Date..mmddyyyy. = as.Date(df$Release.Date..mmddyyyy., 
                                     format="%m/%d/%Y")
# rename columns
names(df)[names(df) == 'Critic.Score..IMDB.x.10.'] = 'Critic.Score'
names(df)[names(df) == 'Run.Time..minutes.'] = 'Run.Time'
names(df)[names(df) == 'Release.Date..mmddyyyy.'] = 'Release.Date'

# create netprofit field
df$Net.Profit = df$Box.Office - df$Budget

# create sentiment field off title
df$Title.Sentiment = (sentimentr::sentiment(df$Title))$sentiment

# add more specific date fields
df$Day.of.Week = weekdays(df$Release.Date)
df$Release.Month = as.factor(months(df$Release.Date))
```


## Abstract
Movie production companies are interested in understanding variables contributing
to a successful film, particularly whether film budget and IMBD score 
are predictive of net profits. Box office data from all 2019 film releases 
was collected and enhanced with additional features to address this question. 

Initially, a linear model was used to explore response associations and establish
a baseline. The initial model displayed poor fit according to the residual standard 
error distribution, warranting a more complicated analysis. Several hierarchical models
were used to address heteroscedastic variance concerns and ultimately a mixed effect
model was applied to observations based on release month, which enhanced estimation 
accuracy for most fixed effects.

Final estimates for 'Budget' and 'Critic.Score' have statistically significant
positive effects on profitability at the 95% level. On average, every $1 increase
in budget yields an additional profit of $2.43 and improving the IMBD score by one 
point leads to a net profit increase of ~$45 million. In addition, augmented field 'Title.Sentiment',
defined as the polarity score of the film's title, showed suprising significance,
indicating a ~$97 million net profit increase per unit gain of sentiment score. 

For production companies, it's not always feasible to create high budget, critically
acclaimed films, but they can control the naming. One actionable recommendation 
is to title movies positively, as audiences seem more inclined to purchase tickets
to these films.

## Dataset Overview & Issues

**Metadata Information**  
All fields used in the analysis are shown in Table 1 with metadata descriptions and
a sample observation. Fields highlighted in yellow are augmentations. 

<!-- metadata desc table -->
```{r, raw-data-fields, include=TRUE, fig.height=4}
# set.seed(79)
set.seed(777)
options(scipen=999)
options(digits = 3)
sample = df %>% slice_sample(n=1)

## refactor table
tbl_df = data.frame(
    Column.Names = colnames(df),
    Column.Description = c(
      "film's title",
      "date of movie release",
      "name of the movie production company",
      "leading actor/actress in the film",
      "second leading actor/actress in the film",
      "third leading actor/actress in the film",
      "director of the film",
      "amount (USD) film made at the box office",
      "budget (USD) of film",
      "run time (minutes) of film",
      "IDMB score of film",
      "full genre",
      "main genre of film",
      "secondary genre of film",
      "number of genres of film",
      "profit (USD) of film",
      "polarity score of movie title",
      "release day",
      "release month"
    ),
    Sample = t(sample)
)
ggtexttable(tbl_df, rows=NULL, cols = c("Field Name", "Description", "Sample"),
                  theme=ttheme("classic", base_size = 7, padding=unit(c(3,3), "mm"))) %>%
  table_cell_font(row = 2:(nrow(tbl_df) + 1), column = 1, face = "bold", size = 6.5) %>%
  table_cell_font(row = 2:(nrow(tbl_df) + 1), column = 2, face = "italic", size = 6.5) %>%
  table_cell_font(row = 2:(nrow(tbl_df) + 1), column = 3, face = "italic", size = 6.5) %>%
  table_cell_bg(row = 14:(nrow(tbl_df) + 1), column=1:3, linewidth = 1,
                fill="#CC9900", color = "black") %>%
  tab_add_title(text = "Table 1: Metadata Information", size = 9, face = "plain",
                padding=unit(c(0,0), "mm"))
```

**Missing Data**  
<!-- - should probably look to remove outliers before doing data imuptation -->
Exploratory data analysis (EDA) revealed a large number of missing data across a 
subset of fields. Figure 1 shows the five fields with missing data. Over 
a third of movie releases are missing "Net.Profit"...the response variable we want to
investigate! Clearly something must be done to address this issue. Bayesian 
imputation methods were investigated to "draw" missing data from candidate
probability distributions derived from available data, but due to the large proportion
of missing data in the response variable this strategy was abandoned in favor
of dropping affected observations. This avoids adding any statistical bias from 
an imputation method, however, this increases variability of results. Before 
proceeding several additional data quality issues were discovered.

<!-- fig.cap='Percent of missing data from specified field. Note columns with no missing data were excluded' -->
<!-- caption not working here  -->
```{r, na-analysis, include=TRUE, fig.height=3, fig.width=5, fig.cap="Missing Data Summary"}
na_count = sapply(df, function(y) sum(length(which(is.na(y)))))

tst = data.frame(na_count) %>%
  filter(na_count > 0)

ggplot(tst, aes(x=row.names(tst), y=na_count/nrow(df))) + 
  geom_bar(stat = 'identity') + 
  labs(x = "Field Name", y = "Fraction Missing", title="Missing Data Summary") +
  ylim(0,0.4) + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_text(aes(label=round(na_count/nrow(df), 3)), position=position_dodge(width=0.9), vjust=-0.25, size = 3)
```

<!-- missing data imputation -->
```{r, missing-data-imputation}
# bayesian imputation with 5 trials
# num_trials = 5
# # prep data for imputation
# raw_imp_df = df %>% select(-Net.Profit, -Title.Sentiment)
# imp_df = mice(raw_imp_df, m = num_trials, print = TRUE)
# # generate mega dataset
# new_df = complete(imp_df, "long")
```

<!-- **Other Data Issues**   -->
**Limited Releases:** There are some additional concerns with the data. First, we have no information
about how box office metrics were collected, such as release duration or the number 
of screening theaters. Films debuting as limited releases have
fewer opportunities to generate box office profits and have skewed revenue numbers.
A known example of a limited release is "The Irishman", shown below. At first 
glance the film appears to have lost over $100 million, but additional research 
indicates the film only released at eight theaters nationally. Because of this 
concern, all Netflix films were dropped from analysis except "Isn't it Romance" 
as secondary research confirmed a full theatrical release.

```{r, include=TRUE, the-irishman-sample, fig.height=1.2}
options(scipen=999)
options(digits = 3)
samp = df %>% 
  filter(Title %in% 
           c("The Irishman", "Avengers: Endgame", "The Lion King", "Frozen II")) %>%
  dplyr::select(Title, Production.Company, Box.Office, Budget, Net.Profit) %>%
  mutate_each(funs(prettyNum(., big.mark=",", format="e")))

samp$Removal.Reason = c("Outlier", "Outlier", "Limited Release", "Outlier")

ggtexttable(samp, rows=NULL, theme=ttheme("classic", base_size = 7, padding=unit(c(3,3), "mm"))) %>%
  table_cell_bg(row = c(2:3, 5), column=5, linewidth = 3, fill="lightgreen", 
                color = "black") %>%
  table_cell_bg(row = 4, column=5, linewidth = 3, fill="#FF9999", 
                color = "black") %>%
  tab_add_title(text = "Table 2: Outlier Sample", size = 9, face = "plain",
                # hjust=-1.75, vjust=1.5,
                padding=unit(c(0,0), "mm"))

## drop netflix films and Outliers
df = df %>%
  filter(!(grepl("Netflix", Production.Company, fixed = TRUE) & Title != "Isn't It Romance")) %>%
  filter(!Title %in% c("Avengers: Endgame", "The Lion King", "Frozen II"))

## create dropped dataset
dropped_df = na.omit(df)
```
<!-- outlier detection -->
**Outliers:** The dataset also contains a number of outliers which can lead to large 
leverage points and/or violate modeling assumptions. One such example is "Avengers:
Endgame", which released as the final installment to a series of films from 
the past decade to much fanfare and the box office numbers numbers are almost
twice as large as the next closest film. It's reasonable to assume that many 
people were motivated by a different set of factors to watch this film and as a 
result it should be be included in the analysis. Other "sequels" with extraneous
factors were also removed (Lion King & Frozen II) for the same reason.

## Variable Selection
The main objective of this analysis is to investigate factors influencing 'Net.Profit'.
Before beginning to build a model the covariance structure of the dataset was 
investigated for films with no missing values. Table 3 displays the correlation
coefficients for numeric fields and indicates mostly weak relationships 
(correlation < 0.5) with the response. Unsurprisingly, 'Box.Office' and 'Budget' 
show the strongest relationship. Figure 2 shows the relationship between the response
and the two main covariates of interest: budget and critical score.

```{r, corrplot, include=TRUE, fig.height=1.8}
## this only looks at complete cases (rows with no NAs)
M = cor(select_if(df, is.numeric), use="complete.obs")

# corr = corrplot(M, type="upper", order="hclust")
tbl = ggtexttable(round(M, 2), theme=ttheme("classic", base_size = 8, padding=unit(c(3,3), "mm")))
tbl = table_cell_bg(tbl, row = 7, column=2:tab_ncol(tbl), linewidth = 5,
                    fill="darkolivegreen1", color = "darkolivegreen4") %>%
  table_cell_bg(row = 2:tab_nrow(tbl), column=7, linewidth = 5,
                    fill="darkolivegreen1", color = "darkolivegreen4") %>%
  tab_add_title(text = "Table 3: Correlation Matrix", size = 9, face = "plain",
                # hjust=-1.75, vjust=1.5
                padding=unit(c(0,0), "mm"))

scatter = ggplot(df, aes(x=Budget/10^6, y=Box.Office/10^6)) + 
  geom_point() +
  geom_abline(intercept = 0, slope = 1, size = 0.5, linetype="dashed") +
  geom_smooth(method = lm, se=TRUE, size=.5, alpha  = .5) + 
  labs(x="Budget (Millions)", y="Box Office (Millions)")

## remove the plot
# ggarrange(tbl, scatter, ncol=1, nrow=2, heights=c(0.7, 1))
tbl
```

<!-- to address the first question of interest -->
```{r, relation-to-profit-plots, include=TRUE, fig.height=2.7, fig.cap="Relationship with Net-Profit"}
# net-profits and budget
p1 = ggplot(df, aes(x=Budget/10^6, y=Net.Profit/10^6)) +
  geom_point() +
  geom_smooth(method = lm, se=TRUE, size=.5, alpha  = .5) + 
  theme(plot.subtitle = element_text(hjust = 0.5)) + 
  labs(subtitle="Budget", y="Net-Profit (Millions)", x="Budget (Millions)")

# net-profits and critical score
p2 = ggplot(df, aes(x=Critic.Score, y=Net.Profit/10^6)) +
  geom_point() +
  geom_smooth(method = lm, se=TRUE, size=.5, alpha  = .5) + 
  labs(subtitle="Critical Score") + 
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        plot.subtitle = element_text(hjust = 0.5))

ggarrange(p1, p2, ncol=2, nrow=1, widths = c(1.1,0.9))
# annotate_figure(plot, top = text_grob("Relationship with Net-Profit", 
#                color = "black", size = 14))
```


## Model Building

<!-- model 1...the base model (no hiearchical) -->
**Standard Regression**  
Before more complex modeling, a baseline linear regression
model was fit on the data. Let $y \in {\rm I\!R}^n$ be a vector of 
movie net profits, $X \in {\rm I\!R}^{n \times 2}$ be a matrix of 
covariates ('Budget' & 'Critic.Score'), and $\beta_0, \beta$ be regression coefficients
and intercept respectively. The model can be formalized as follows:

$$y = \beta_0 + X\beta + \epsilon \text{ , where }\epsilon_{iid} \sim  N(0, \sigma^2I)$$
<!-- fitting the model -->
```{r, model-1-budget-critical-score}
# with dropped data
model_1 = lm(formula = Net.Profit ~ Budget + Critic.Score, data=dropped_df)
summary(model_1)
plot(model_1)
```

The results of the model fit including coefficient estimates, uncertainty 
quantification via 95% confidence interval and p-values are shown in Table 4. All
coefficients, including the intercept are shown to be statistically significant,
but standard errors (SE) are large for Critic.Score and intercept.

<!-- present results -->
```{r, include=TRUE, linear-model-summary-tbl, fig.height=1}
# create results data frames
rslts = data.frame(
  Coeffs = names(model_1$coefficients),
  Estimate = model_1$coefficients,
  SE = sqrt(diag(vcov(model_1))),
  p = summary(model_1)$coefficients[,4],
  CI_Lower = confint(model_1)[,1],
  CI_Upper = confint(model_1)[,2]
)
options(scipen = 1)
options(digits = 3)
# format the results with commas and appropriate rounding
rslts = rslts %>% mutate_each(funs(prettyNum(., big.mark=",", format="e")))

# create tables and present
ggtexttable(rslts, rows = NULL, 
            cols = c("Coefficient", "Estimate (USD)","SE (USD)", "P-Value", 
                     "CI Lower (2.5%)", "CI Upper (97.5%)"),
                  theme=ttheme("classic", base_size = 8, padding=unit(c(3,3), "mm")))  %>%
  tab_add_title(text = "Table 4: Initial Results", size = 9, face = "plain",
                # hjust=-1.15, vjust=1.5
                padding=unit(c(0,0), "mm"))
```

<!-- second summary table -->
```{r, include=TRUE, fig.height=0.45}
summ = data.frame(
  Obs = nrow(dropped_df),
  Residual_SE = sqrt(sum(model_1$residuals^2)/model_1$df.residual),
  R_2 = summary(model_1)$r.squared,
  Adjusted_R_2 = summary(model_1)$adj.r.squared
)
summ = summ %>% mutate_each(funs(prettyNum(., big.mark=",", format="e")))

ggtexttable(summ, rows = NULL,
            cols = c("Observations", "Residual Std. Error", "R^2", "Adjusted R^2"),
                  theme=ttheme("classic", base_size = 8, padding=unit(c(3,3), "mm")))
```

Based on the high SEs, the model fit was assessed. The scale location plot is the 
most interesting as it addresses the large differences in film net profits which
were left as raw dollar amounts. Clearly, as the fitted value increases the 
standardized residual increases which is a violation of the linear model homoscedastic
variance assumption. Before moving to a more complex modeling technique additional
covariates were added to try and improve the baseline regression.

<!-- this plot basically suggests a log transform of Budget...taken
the log transform of both y and x make the residual fit so much better
-->

<!-- assessing model fit -->
```{r, assess-lm-fit, include=TRUE, fig.height=3.75, fig.cap="Standardized Residual Plot"}
# par(mfrow = c(1, 2))
# plot(model_1, which=c(1))
plot(model_1, which=c(3))
```

<!--
      end model 1 
-->

**Adding Additional Covariates to the Standard Regression**  
The correlation matrix in Table 3 suggests 'Run.Time' and 'Title.Sentiment'
may be good predictors to add to the model. Models with each additional covariate
were compared to the reference model using an F-test to determine inclusion 
criteria. In addition to the two numeric covariates, 'Day.of.Week' and 'Genre.1'
were also investigated. F-test summaries are provided below and based on these
results only 'Title.Sentiment' was added to the model as closer inspection of 
'Day.of.Week' revealed a high variance estimate from Tuesday (only has three observations).

<!-- covariate experimentation -->
```{r, include=TRUE, f-test-cov-expansion, fig.height=1.4}
## add sentiment
model_1_b = lm(formula = Net.Profit ~ Budget + Critic.Score + Title.Sentiment, data=dropped_df)
# summary(model_1_b)
tstb = anova(model_1, model_1_b)

## add day of week
model_1_c = lm(formula = Net.Profit ~ Budget + Critic.Score + Day.of.Week, data=dropped_df)
# summary(model_1_c)
tstc = anova(model_1, model_1_c)

## add run time (interaction term??)
model_1_d = lm(formula = Net.Profit ~ Budget + Critic.Score + Run.Time, data=dropped_df)
# summary(model_1_d)
tstd = anova(model_1, model_1_d)

## add genre.1
model_1_e = lm(formula = Net.Profit ~ Budget + Critic.Score + Genre.1 , data=dropped_df)
# summary(model_1_e)
tste = anova(model_1, model_1_e)

### build results table
tst_rslts = data.frame(
  covar = c("Reference Model", "Title.Sentiment", "Day.of.Week", "Run.Time", "Genre.1"),
  DF = c(tstb$Res.Df, tstc$Res.Df[-1], tstd$Res.Df[-1], tste$Res.Df[-1]),
  RSS = c(tstb$RSS, tstc$RSS[-1], tstd$RSS[-1], tste$RSS[-1]),
  SumSq = c(tstb$`Sum of Sq`, tstc$`Sum of Sq`[-1], tstd$`Sum of Sq`[-1], tste$`Sum of Sq`[-1]),
  F_ = c(tstb$F, tstc$F[-1], tstd$F[-1], tste$F[-1]),
  P_ = c(tstb$`Pr(>F)`, tstc$`Pr(>F)`[-1], tstd$`Pr(>F)`[-1], tste$`Pr(>F)`[-1])
)

ggtexttable(tst_rslts, rows = NULL,
            cols = c("Added Covariate", "Df", "RSS", "RSS Reduction from Ref.",
                     "Test-Statistic (F)", "P-Value"),
            theme=ttheme("classic", base_size = 8, padding=unit(c(3,3), "mm"))) %>%
  tab_add_title(text = "Table 5: F-Test Results", size = 9, face = "plain", 
                # hjust=-1.4, vjust=1.5
                padding=unit(c(0,0), "mm"))
```

The final "baseline" linear regression model results are shown in Table 6.

<!-- fit final model -->
```{r, include=TRUE, final-linear-model, fig.height=1.25}
# create results data frames
rslts = data.frame(
  Coeffs = names(model_1_b$coefficients),
  Estimate = model_1_b$coefficients,
  SE = sqrt(diag(vcov(model_1_b))),
  p = summary(model_1_b)$coefficients[,4],
  CI_Lower = confint(model_1_b)[,1],
  CI_Upper = confint(model_1_b)[,2]
)
options(scipen = 1)
options(digits = 3)
# format the results with commas and appropriate rounding
rslts = rslts %>% mutate_each(funs(prettyNum(., big.mark=",", format="e")))

# create tables and present
ggtexttable(rslts, rows = NULL,
                      cols = c("Coefficient", "Estimate (USD)", "SE (USD)",
                               "P-Value", "CI Lower (2.5%)", "CI Upper (97.5%)"),
                  theme=ttheme("classic", base_size = 8, padding=unit(c(3,3), "mm")))  %>%
  tab_add_title(text = "Table 6: Final Baseline Reference Results", size = 9, face = "plain",
                # hjust=-0.85, vjust=1.5
                padding=unit(c(0,0), "mm"))
```

```{r, include=TRUE, final-summary-table, fig.height=0.45}
summ = data.frame(
  Obs = nrow(dropped_df),
  Residual_SE = sqrt(sum(model_1_b$residuals^2)/model_1_b$df.residual),
  R_2 = summary(model_1_b)$r.squared,
  Adjusted_R_2 = summary(model_1_b)$adj.r.squared
)
summ = summ %>% mutate_each(funs(prettyNum(., big.mark=",", format="e")))

ggtexttable(summ, rows = NULL,
            cols = c("Observations", "Residual Std. Error", "R^2", "Adjusted R^2"),
                  theme=ttheme("classic", base_size = 8, padding=unit(c(3,3), "mm")))
```

This final model improves the residual standard error estimate from the reference
model, but is still a relatively poor fit (high coefficient SEs). To try and reduce
these values further hierarchical modeling is explored.

<!-- end linear model 1 with additional covariates -->

**Hierarchical Modeling**  
To perform an adequate hierarchical analysis a "clustering" category is needed.
At first glance, 'Genre' shows promise, but many films are recorded with multiple 
or unique even entries with limited overlap. Table 7 displays the
number of films with multiple genres. The median film has more than one genre, 
making grouping more challenging.

<!-- multiple genres table -->
```{r, include=TRUE, genre-table, fig.height=1.2}
ggtexttable(df %>% 
              group_by(Num.Genres) %>%
              count(),
            rows = NULL,
            cols = c("Number of Genres by Film", "Number of Observations"),
                  theme=ttheme("classic", base_size = 8, padding=unit(c(2.5,2.5), "mm"))) %>%
  tab_add_title(text = "Table 7: Genre Clustering", size = 9, face = "plain", 
                # hjust=-0.6, vjust=1.5
                padding=unit(c(0,0), "mm"))
```

One could circumvent this by using the first listed entry as the primary genre
and proceed with analysis, but there are still quite a few clusters with few 
observational data points shown in Figure 4. A proposed solution is to 
categorize similar genres into a hierarchy for analysis. For example, maybe "War"
and "Action" involve enough similarity to warrant combining into one super-category,
however, extensive domain knowledge may be required to infer less obvious groupings.
This procedure might introduce statistical bias that could alter results.

```{r, include=TRUE, genre-plot, fig.height=3.5, fig.cap="Count of Films by Genre.1"}
# df %>% group_by(Genre.1) %>% count() %>% arrange(n)
df_group_mn = (df %>% 
                 group_by(Genre.1) %>%
                 count() %>%
                 ungroup() %>% 
                 summarize(mean = median(n)))[[1]]
ggplot(df %>% 
         group_by(Genre.1) %>% 
         count(), aes(x=n, y=reorder(Genre.1, n))) +
  geom_bar(stat="identity") + 
  labs(y="Genre") + 
  theme(axis.title.y=element_blank()) +
  geom_vline(xintercept = df_group_mn, color="red", linetype="dashed", size=1) +
  geom_text(aes(label = "Median", x = 4, y = -Inf), vjust = 1) + 
  coord_cartesian(clip = 'off')
```

A more objective hierarchy is needed and one hypothesis is that audiences may
be sensitive to the seasonality of a film's release. For example, during the
winter months individuals might be more likely to attend a new film because there
are less alternative outdoor activities compared to other seasons. Production companies
might alter a film's budget accordingly. Figure 5 displays linear regression fits
contrasting the group effect. Clearly, there are differences in Net.Profit trends
across groups, warranting the hierarchical model.


```{r, include=TRUE, motivating-hiearchy, fig.align='center', fig.height=3, fig.cap="Motivating the Hierarchical Model"}
p1 = ggplot(data=df,
       aes(x=Critic.Score, y=Net.Profit/10^6)) +
  geom_point(size = 1.2, alpha=.9) + 
  # theme_minimal() +
  theme(legend.position = "none", plot.subtitle = element_text(hjust = 0.5)) +
  # add regression line
  geom_smooth(method = lm, se=FALSE, size=.5, alpha  = .5) +
  labs(subtitle="No Grouping", x="Critic Score", y="Net-Profit ($ Millions)")
  
p2 = ggplot(data=df,
       aes(x=Critic.Score, y=Net.Profit/10^6, col=Release.Month)) +
  geom_point(size = 1.2, alpha=.9) + 
  # theme_minimal() +
  theme(legend.position = "none") +
  # add regression line
  geom_smooth(method = lm, se=FALSE, size=.5, alpha  = .5) +
  labs(subtitle = "Grouped by Release Month", x="Critic Score") +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        plot.subtitle = element_text(hjust = 0.5))

ggarrange(p1, p2, ncol=2, nrow=1)
# annotate_figure(plot, top = text_grob("Motivating the Model", 
#                color = "black", size = 14))
```

\newpage

Several hierarchical models were tried with the data. First, a random
intercept on release month was added. The model can be described using the same
notation as the final reference linear model with new parameter 
$\alpha_{j} \in {\rm I\!R}^ {12 \times 1}$ denoting the random effect for release 
month $j$.

$$y = \beta_0 + \alpha_{j} +  X\beta + \epsilon \text{ , where }\epsilon_{iid} \sim  N(0, \sigma^2I)$$

This model detects very little differences with the addition of the random effect 
and all parameter estimates are almost identical to the reference linear model.

```{r}
mixed_model_1 = lmer(formula = Net.Profit ~ Budget +  Critic.Score + Title.Sentiment + 
                     (1 | Release.Month), data=dropped_df)
# summary(mixed_model_1)
# REsim(mixed_model_1)             # mean, median and sd of the random effect estimates
# plotREsim(REsim(mixed_model_1))  # plot the interval estimates
```

```{r, include=TRUE, fig.height=1.3}
tst = coef(summary(mixed_model_1))
conf_results = confint(mixed_model_1, oldNames=FALSE)

rslts = data.frame(
  Coeffs = names(tst[,1]),
  Estimate = tst[,1],
  SE = tst[,2],
  p = tst[,5],
  CI_Lower = conf_results[3:nrow(conf_results),1],
  CI_Upper = conf_results[3:nrow(conf_results),2]
)
options(scipen = 1)
options(digits = 3)
# format the results with commas and appropriate rounding
rslts = rslts %>% mutate_each(funs(prettyNum(., big.mark=",", format="e")))

# create tables and present
ggtexttable(rslts, rows = NULL,
            cols = c("Coefficient", "Estimate (USD)","SE (USD)", "P-Value",
                     "CI Lower (2.5%)", "CI Upper (97.5%)"),
                  theme=ttheme("classic", base_size = 8, padding=unit(c(3,3), "mm")))  %>%
  tab_add_title(text = "Table 8: Mixed Model 1 Results", size = 9, face = "plain",
                # hjust=-1.15, vjust=1.5
                padding=unit(c(0,0), "mm"))
```

```{r, include=TRUE, fig.height=0.6}
rslts = data.frame(
  Coeffs = names(conf_results[1:2,1]),
  CI_Lower = conf_results[1:2,1],
  CI_Upper = conf_results[1:2,2]
)
options(scipen = 1)
options(digits = 3)
# format the results with commas and appropriate rounding
rslts = rslts %>% mutate_each(funs(prettyNum(., big.mark=",", format="e")))

# create tables and present
ggtexttable(rslts, rows = NULL, 
            cols = c("Coefficient", "CI Lower (2.5%)", "CI Upper (97.5%)"),
                  theme=ttheme("classic", base_size = 8, padding=unit(c(3,3), "mm")))
```

Clearly the model doesn't detect any differences under the first model and ideally,
we would like to add random slope terms for all covariates, but unfortunately, 
the small sample size can lead to boundary fits and singularity for too many
parameters. 

The final hierarchical model is defined with similar notation as follows. To 
avoid boundary conditions, the random intercept is removed and instead estimate
the random slope for 'Critic.Score', $\gamma_{j[i]}$ the random effect for 
critic score of film $c_i$, in addition to all other parameters from the
baseline reference linear model.

$$y_{ij} = \beta_0  + X\beta  + \gamma_{j[i]} c_i + \epsilon_{ij} \ \text{ where } \epsilon_{ij} \sim N(0, \sigma^2)$$

```{r}
mixed_model_2 = lmer(formula = Net.Profit ~ Budget +  Critic.Score + Title.Sentiment + 
                     (0 + Critic.Score | Release.Month), data=dropped_df)
# summary(mixed_model_2)
# REsim(mixed_model_2)             # mean, median and sd of the random effect estimates
# plotREsim(REsim(mixed_model_2))  # plot the interval estimates
```

Accounting for this new parameter alters the parameter estimates compared to the 
baseline reference model, but does not dramatically change the coefficient SE 
estimates...a disappointment. Comparing Table 9 to Table 6 the estimate for 
'Budget' is identical, but 'Critic.Score' and 'Title.Sentiment' see an increase
with lower p-values that almost nudge 'Title.Sentiment' into a statistically 
significant effect at the 95% level.


```{r, include=TRUE, fig.height=1.3}
tst = coef(summary(mixed_model_2))
conf_results = confint(mixed_model_2, oldNames=FALSE)

rslts = data.frame(
  Coeffs = names(tst[,1]),
  Estimate = tst[,1],
  SE = tst[,2],
  p = tst[,5],
  CI_Lower = conf_results[3:nrow(conf_results),1],
  CI_Upper = conf_results[3:nrow(conf_results),2]
)
options(scipen = 1)
options(digits = 3)
# format the results with commas and appropriate rounding
rslts = rslts %>% mutate_each(funs(prettyNum(., big.mark=",", format="e")))

# create tables and present
ggtexttable(rslts, rows = NULL,
            cols = c("Coefficient", "Estimate (USD)","SE (USD)", "P-Value",
                     "CI Lower (2.5%)", "CI Upper (97.5%)"),
                  theme=ttheme("classic", base_size = 8, padding=unit(c(3,3), "mm")))  %>%
  tab_add_title(text = "Table 9: Mixed Model 2 Results", size = 9, face = "plain",
                # hjust=-1.15, vjust=1.5
                padding=unit(c(0,0), "mm"))
```

```{r, include=TRUE, fig.height=0.6}
rslts = data.frame(
  Coeffs = names(conf_results[1:2,1]),
  CI_Lower = conf_results[1:2,1],
  CI_Upper = conf_results[1:2,2]
)
options(scipen = 1)
options(digits = 3)
# format the results with commas and appropriate rounding
rslts = rslts %>% mutate_each(funs(prettyNum(., big.mark=",", format="e")))

# create tables and present
ggtexttable(rslts, rows = NULL, 
            cols = c("Coefficient", "CI Lower (2.5%)", "CI Upper (97.5%)"),
                  theme=ttheme("classic", base_size = 8, padding=unit(c(3,3), "mm")))
```


```{r, include=TRUE, fig.height=3.3, fig.cap="Interval Estimates for the Random Effect"}
plotREsim(REsim(mixed_model_2))  # plot the interval estimates
```



```{r, model-2}
## with droppped data
## random intercept
# mixed_model_1 = lmer(formula = Net.Profit ~ Budget + Critic.Score + Title.Sentiment + (1 | Release.Month)
#                      , data=dropped_df)
# summary(mixed_model_1)
# # shows no difference between random effects
# mixedup::extract_random_effects(mixed_model_1)
# plot(mixed_model_1)
# ## residual density plot looks identical
# plot(density(residuals(mixed_model_1)))
```



## Conclusions, Limitations & Enhancements
In this analysis factors affecting box office success were evaluated. Before 
modeling, covariates were cleaned, analyzed and enhanced to better understand
dependence. Roughly 100 observations were dropped from the analysis related to 
data quality. Standard linear regression and mixed effect models were evaluated
to assess the statistical significance of covariates on profitability. 
'Budget' and 'Critic.Score' were found to have significant positive effects at the 95% 
confidence level, while 'Title.Sentiment' was found to be significant at the 90% 
confidence level, adding a suprising positive effect. However, it's important 
to note that more analysis is needed to determine the causal relationship between 
'Title.Sentiment' and net profitability as only associativity is assessed. In addition,
there are several limitations of this analysis and the main concerns are addressed.

First, the sample contains a large fraction of missing data, is relatively small, 
and only contains a single year of observations which might not be representative 
of overall film releases. 
Second, no knowledge of the data collection procedure is provided and raises concerns
about the use of overall profit as a metric which may be misleading. Outlier 
detection revealed several films with excessive losses that released limitedly,
but more lower budget limited releases may be present. To combat this, one could 
code profitability as a binary response indicator, $\{0,1\}$, and fit a logistic 
regression on the new dependent variable, minimizing the influence of any outliers
and potentially make the analysis more robust. However, the ability to quantify 
direct covariate effect on overall profit would be lost, which feels relevant to
any profit evaluation. 
Third, there is no obvious clustering field for a hierarchical 
model. Spending more time to manually group like genres may yield higher standard
error reduction and more accurate coefficient estimates.
Lastly, more analysis is needed to determine the causal relationship between 
'Title.Sentiment' and net profitability as the current report analyzes association.

<!-- ## Appendix -->

<!-- how many rows have at least 1 na? -->
```{r, na-by-row}
# list of na count for each row
# df_na = rowSums(is.na(df))
# pander(data.frame(
#   Number.of.NAs = c(0,1,2,3),
#   Row.Count = c(length(df_na[df_na == 0]), length(df_na[df_na == 1]), length(df_na[df_na == 2]), length(df_na[df_na == 3])),
#   Percent = c(length(df_na[df_na == 0])/length(df_na), length(df_na[df_na == 1])/length(df_na), length(df_na[df_na == 2])/length(df_na), length(df_na[df_na == 3])/length(df_na))
# ))
## with at least 1 missing value ~= 36%
# length(df_na[df_na != 0])/length(df_na)
```

```{r, eda}
## outlier box plots
# ggplot(df, aes(x=Box.Office)) + geom_boxplot()
# ggplot(df, aes(x=Net.Profit)) + geom_boxplot()

## plot of net profit vs. budget
# ggplot(df, aes(x=Budget, y=Net.Profit)) + 
#   geom_point() +
#   geom_hline(yintercept=0)
```

```{r, qual-corr}
# corr_df = df %>% na.omit() %>% 
#   select(Genre.1, Title.Sentiment)
# corr_df$Genre.1 = as.character(corr_df$Genre.1)
# 
# # given two column indices return categorical correlation 
# compute_corr = function(sample, i, j){
#   cramerV(table(sample[,i], sample[,j]))
# }
# 
# # create table for report
# corr_matrix = matrix(nrow = ncol(corr_df), ncol = ncol(corr_df))
# rownames(corr_matrix) = colnames(corr_df)
# colnames(corr_matrix) = colnames(corr_df)
# 
# for(i in 1:ncol(corr_df)){
#   for(j in 1:ncol(corr_df)){
#      corr_matrix[i,j] = compute_corr(corr_df, i,j)
#   }
# }
# 
# pander(corr_matrix, caption="Correlation Matrix")
```

```{r, density-plots}
# plot(density(df$Box.Office, na.rm=TRUE))
# lines(density(df$Budget, na.rm=TRUE), col="red")
# plot(density(df$Budget, na.rm=TRUE), col="red", main="Density Overlay (BoxOffice and Budget)")
# lines(density(df$Box.Office, na.rm=TRUE), col="green")

# ## log densities
# plot(density(log(df$Budget), na.rm=TRUE), col="red", main="Density Overlay (BoxOffice and Budget)")
# lines(density(log(df$Box.Office), na.rm=TRUE), col="green")
```


